{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa88c77ab81248e3b7d06b044835cef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a3db94b21324307af0bc93435de7637",
              "IPY_MODEL_52e4afdf6c334a71b308f909d794e68e",
              "IPY_MODEL_13152e158657472a93e9b40c1a914384"
            ],
            "layout": "IPY_MODEL_d544de70ea7b4af08c944013a845843f"
          }
        },
        "4a3db94b21324307af0bc93435de7637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bfd6ff7c663481095201d0542ff7023",
            "placeholder": "​",
            "style": "IPY_MODEL_c4582ab0f18e4e4b8eee5f1f46f7c9a4",
            "value": "Batches: 100%"
          }
        },
        "52e4afdf6c334a71b308f909d794e68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919cfde28fa44a7da73e1c4610ebde4f",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60d1ef54cb9d403a837d0042189566f0",
            "value": 21
          }
        },
        "13152e158657472a93e9b40c1a914384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5ef158d42c14d47a7f45319403cdb74",
            "placeholder": "​",
            "style": "IPY_MODEL_91d19edb636d4e83a9cb992e1353020f",
            "value": " 21/21 [00:02&lt;00:00, 21.39it/s]"
          }
        },
        "d544de70ea7b4af08c944013a845843f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bfd6ff7c663481095201d0542ff7023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4582ab0f18e4e4b8eee5f1f46f7c9a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "919cfde28fa44a7da73e1c4610ebde4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60d1ef54cb9d403a837d0042189566f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5ef158d42c14d47a7f45319403cdb74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91d19edb636d4e83a9cb992e1353020f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4-mLDt_b9j4",
        "outputId": "918f8698-83ef-44f9-df9a-09b55c187df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All packages imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Installation and Imports\n",
        "\n",
        "!pip install -q datasets sentence-transformers transformers faiss-cpu numpy pandas\n",
        "\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import faiss\n",
        "import re\n",
        "\n",
        "print(\"All packages imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 1. Configuration\n",
        "# =============================================================================\n",
        "\n",
        "RAG_CONFIG = {\n",
        "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"vector_dim\": 384,\n",
        "    \"chunk_size\": 600,\n",
        "    \"batch_size\": 64,\n",
        "    \"initial_retrieval\": 30,  # Retrieve more for reranking\n",
        "    \"final_top_k\": 5,  # After reranking\n",
        "    \"query_variations\": 2,\n",
        "    \"context_budget\": 2000\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "for k, v in RAG_CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dilb-ksqe6mJ",
        "outputId": "a4e651f3-3baa-45d6-8a72-b84a9939a0f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CONFIGURATION\n",
            "================================================================================\n",
            "  embedding_model: sentence-transformers/all-MiniLM-L6-v2\n",
            "  vector_dim: 384\n",
            "  chunk_size: 600\n",
            "  batch_size: 64\n",
            "  initial_retrieval: 30\n",
            "  final_top_k: 5\n",
            "  query_variations: 2\n",
            "  context_budget: 2000\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2. Load Data\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nLoading dataset...\")\n",
        "\n",
        "corpus = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
        "qa_set = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")\n",
        "\n",
        "passages = corpus[\"passages\"]\n",
        "test_questions = qa_set[\"test\"]\n",
        "\n",
        "# Limit for experimentation\n",
        "MAX_DOCS = 1000\n",
        "passages = passages.select(range(min(MAX_DOCS, len(passages))))\n",
        "\n",
        "print(f\"Working with {len(passages)} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRf2w5Gog9Y1",
        "outputId": "73f776b2-8519-47cb-e7b4-177d5cb2a67e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with 1000 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3. Chunking\n",
        "# =============================================================================\n",
        "\n",
        "def split_into_chunks(text: str, size: int) -> List[str]:\n",
        "    \"\"\"Split text into fixed character chunks.\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    return [text[i:i+size] for i in range(0, len(text), size)]\n",
        "\n",
        "# Create chunks\n",
        "all_chunks = []\n",
        "for doc_idx, doc in enumerate(passages):\n",
        "    text_content = doc.get(\"passage\", \"\")\n",
        "    chunks = split_into_chunks(text_content, RAG_CONFIG[\"chunk_size\"])\n",
        "\n",
        "    for chunk_idx, chunk_text in enumerate(chunks):\n",
        "        all_chunks.append({\n",
        "            \"id\": f\"{doc_idx}-{chunk_idx}\",\n",
        "            \"text\": chunk_text,\n",
        "            \"doc_id\": doc_idx\n",
        "        })\n",
        "\n",
        "print(f\"Created {len(all_chunks)} chunks\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE8uvymIg_sG",
        "outputId": "4097959b-0b9d-4e68-d281-93b347121841"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 1289 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 4. Generate Embeddings\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nGenerating embeddings...\")\n",
        "\n",
        "encoder = SentenceTransformer(RAG_CONFIG[\"embedding_model\"])\n",
        "\n",
        "texts = [c[\"text\"] for c in all_chunks]\n",
        "vectors = encoder.encode(\n",
        "    texts,\n",
        "    batch_size=RAG_CONFIG[\"batch_size\"],\n",
        "    show_progress_bar=True,\n",
        "    normalize_embeddings=True,\n",
        "    convert_to_numpy=True\n",
        ").astype(\"float32\")\n",
        "\n",
        "print(f\"Embeddings shape: {vectors.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "aa88c77ab81248e3b7d06b044835cef9",
            "4a3db94b21324307af0bc93435de7637",
            "52e4afdf6c334a71b308f909d794e68e",
            "13152e158657472a93e9b40c1a914384",
            "d544de70ea7b4af08c944013a845843f",
            "1bfd6ff7c663481095201d0542ff7023",
            "c4582ab0f18e4e4b8eee5f1f46f7c9a4",
            "919cfde28fa44a7da73e1c4610ebde4f",
            "60d1ef54cb9d403a837d0042189566f0",
            "e5ef158d42c14d47a7f45319403cdb74",
            "91d19edb636d4e83a9cb992e1353020f"
          ]
        },
        "id": "p_WQmhZ8hC4w",
        "outputId": "5ac1a846-948f-4fd3-a0d4-b4a8fdfc50da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa88c77ab81248e3b7d06b044835cef9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (1289, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# 5. Build FAISS Index\n",
        "# =============================================================================\n",
        "\n",
        "faiss_index = faiss.IndexFlatIP(RAG_CONFIG[\"vector_dim\"])\n",
        "faiss_index.add(vectors)\n",
        "\n",
        "print(f\"FAISS index contains {faiss_index.ntotal} vectors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbU1Dhx-kxzR",
        "outputId": "f1fff97b-e19b-41f3-f1f2-fb3c009a3772"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index contains 1289 vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 6. FLAN-T5 Setup\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nLoading Flan-T5...\")\n",
        "\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "llm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "text_gen = pipeline(\"text2text-generation\", model=llm_model, tokenizer=llm_tokenizer)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Normalize text.\"\"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
        "    return text\n",
        "\n",
        "print(\"LLM ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XCrJH4Fk3MF",
        "outputId": "3bd967b6-7832-41b6-c935-e75add2d3aa0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Flan-T5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 7. Enhancement #1: Query Rewriting\n",
        "# =============================================================================\n",
        "\n",
        "def generate_query_rewrites(query: str, num_rewrites: int = RAG_CONFIG[\"query_variations\"]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate query variations using LLM.\n",
        "    \"\"\"\n",
        "    cleaned = clean_text(query)\n",
        "\n",
        "    rewrite_templates = [\n",
        "        f\"Paraphrase this question: {cleaned}\",\n",
        "        f\"Reformulate this query: {cleaned}\",\n",
        "        f\"Express this differently: {cleaned}\"\n",
        "    ]\n",
        "\n",
        "    all_queries = [cleaned]  # Include original\n",
        "\n",
        "    for template in rewrite_templates[:num_rewrites]:\n",
        "        try:\n",
        "            output = text_gen(\n",
        "                template,\n",
        "                max_new_tokens=64,\n",
        "                num_beams=2,\n",
        "                do_sample=False\n",
        "            )[0][\"generated_text\"].strip()\n",
        "\n",
        "            if output and len(output) > 10 and output != cleaned:\n",
        "                all_queries.append(clean_text(output))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Deduplicate\n",
        "    unique_queries = []\n",
        "    seen = set()\n",
        "    for q in all_queries:\n",
        "        normalized = q.lower()\n",
        "        if normalized not in seen:\n",
        "            seen.add(normalized)\n",
        "            unique_queries.append(q)\n",
        "\n",
        "    return unique_queries\n"
      ],
      "metadata": {
        "id": "t5u1F6Isk1ql"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 8. Initial Retrievel\n",
        "# =============================================================================\n",
        "\n",
        "def initial_retrieval(query: str, top_k: int = RAG_CONFIG[\"initial_retrieval\"]) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Retrieve candidates using query rewriting.\n",
        "    \"\"\"\n",
        "    query_list = generate_query_rewrites(query)\n",
        "\n",
        "    print(f\"\\nQuery variations ({len(query_list)}):\")\n",
        "    for i, q in enumerate(query_list, 1):\n",
        "        print(f\"  {i}. {q}\")\n",
        "\n",
        "    # Encode all query variations\n",
        "    query_vecs = encoder.encode(\n",
        "        query_list,\n",
        "        normalize_embeddings=True,\n",
        "        convert_to_numpy=True\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    # Search with each variation\n",
        "    seen_indices = {}\n",
        "\n",
        "    for qvec in query_vecs:\n",
        "        distances, indices = faiss_index.search(np.array([qvec]), top_k)\n",
        "\n",
        "        for dist, idx in zip(distances[0], indices[0]):\n",
        "            # Keep highest score for each chunk\n",
        "            if idx not in seen_indices or dist > seen_indices[idx][\"score\"]:\n",
        "                seen_indices[idx] = {\n",
        "                    \"index\": int(idx),\n",
        "                    \"score\": float(dist),\n",
        "                    \"text\": all_chunks[idx][\"text\"],\n",
        "                    \"chunk_id\": all_chunks[idx][\"id\"]\n",
        "                }\n",
        "\n",
        "    # Sort by score\n",
        "    candidates = sorted(seen_indices.values(), key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "    print(f\"Retrieved {len(candidates)} unique candidates\")\n",
        "\n",
        "    return candidates[:top_k]\n"
      ],
      "metadata": {
        "id": "fHLbbBYqqufH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 9. Enhancement #2: Cross-Encoder Reranking\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nLoading CrossEncoder for reranking...\")\n",
        "\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", max_length=512)\n",
        "\n",
        "print(\"Reranker ready\")\n",
        "\n",
        "def rerank_candidates(query: str, candidates: List[dict], top_k: int = RAG_CONFIG[\"final_top_k\"]) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Rerank using CrossEncoder.\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return []\n",
        "\n",
        "    # Prepare query-passage pairs\n",
        "    pairs = [(query, cand[\"text\"]) for cand in candidates]\n",
        "\n",
        "    # Get reranking scores\n",
        "    rerank_scores = reranker.predict(\n",
        "        pairs,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=False,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    # Combine with original candidates\n",
        "    for i, cand in enumerate(candidates):\n",
        "        cand[\"rerank_score\"] = float(rerank_scores[i])\n",
        "\n",
        "    # Sort by rerank score\n",
        "    reranked = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "    print(f\"Reranked to top {top_k}\")\n",
        "\n",
        "    return reranked[:top_k]"
      ],
      "metadata": {
        "id": "lkq07S2nlGeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbaea915-137f-4019-de83-78afe4ffeed8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading CrossEncoder for reranking...\n",
            "Reranker ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 10. Context Assembly\n",
        "# =============================================================================\n",
        "\n",
        "def build_context(chunks: List[dict], budget: int = RAG_CONFIG[\"context_budget\"]) -> tuple:\n",
        "    \"\"\"\n",
        "    Build context with citation markers.\n",
        "    \"\"\"\n",
        "    context_parts = []\n",
        "    citations = []\n",
        "    chars_used = 0\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_id = chunk[\"chunk_id\"]\n",
        "        score = chunk[\"rerank_score\"]\n",
        "        text = chunk[\"text\"]\n",
        "\n",
        "        # Add citation marker\n",
        "        marker = f\"[{chunk_id} | score: {score:.3f}]\"\n",
        "        segment = f\"{marker}\\n{text}\"\n",
        "\n",
        "        # Check budget\n",
        "        if chars_used + len(segment) > budget:\n",
        "            remaining = budget - chars_used\n",
        "            if remaining > 100:\n",
        "                context_parts.append(segment[:remaining] + \"...\")\n",
        "                citations.append({\"id\": chunk_id, \"score\": score})\n",
        "            break\n",
        "\n",
        "        context_parts.append(segment)\n",
        "        citations.append({\"id\": chunk_id, \"score\": score})\n",
        "        chars_used += len(segment)\n",
        "\n",
        "    full_context = \"\\n\\n\".join(context_parts)\n",
        "    return full_context, citations\n"
      ],
      "metadata": {
        "id": "gIB-uQj4lNj0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 11. Answer Generation\n",
        "# =============================================================================\n",
        "\n",
        "def build_prompt(context: str, question: str) -> str:\n",
        "    \"\"\"\n",
        "    Persona prompt (best from Step 3).\n",
        "    \"\"\"\n",
        "    return f\"\"\"You are a subject matter expert. Use only the context.\n",
        "If the answer is not in the context, say 'I don't know'. Be direct.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "def generate_rag_answer(question: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: query rewriting + retrieval + reranking + generation.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Step 1: Initial retrieval with query rewriting\n",
        "    candidates = initial_retrieval(question)\n",
        "\n",
        "    # Step 2: Rerank candidates\n",
        "    reranked = rerank_candidates(question, candidates)\n",
        "\n",
        "    # Step 3: Build context\n",
        "    context, refs = build_context(reranked)\n",
        "\n",
        "    print(f\"\\nContext assembled: {len(context)} chars from {len(refs)} chunks\")\n",
        "\n",
        "    # Step 4: Generate answer\n",
        "    prompt = build_prompt(context, question)\n",
        "\n",
        "    answer = text_gen(\n",
        "        prompt,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=False\n",
        "    )[0][\"generated_text\"].strip()\n",
        "\n",
        "    return answer, refs"
      ],
      "metadata": {
        "id": "U-VsRF_rq7TJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 12. Test with 2 questions\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"─\" * 80)\n",
        "print(\"TESTING RAG SYSTEM - MULTIPLE QUERIES\")\n",
        "print(\"─\" * 80)\n",
        "\n",
        "# Select first two test questions\n",
        "test_questions_list = [\n",
        "    test_questions[0][\"question\"],\n",
        "    test_questions[1][\"question\"]\n",
        "]\n",
        "\n",
        "for q_num, test_q in enumerate(test_questions_list, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TEST QUERY #{q_num}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Query: {test_q}\")\n",
        "\n",
        "    answer, citations = generate_rag_answer(test_q)\n",
        "\n",
        "    print(f\"\\n{'─'*80}\")\n",
        "    print(\"RESULTS\")\n",
        "    print(f\"{'─'*80}\")\n",
        "\n",
        "    print(f\"\\nAnswer: {answer}\")\n",
        "\n",
        "    print(\"\\nSource Citations:\")\n",
        "    print(\"┌──────┬─────────────┬──────────────┐\")\n",
        "    print(\"│ Rank │  Chunk ID   │   Relevance  │\")\n",
        "    print(\"├──────┼─────────────┼──────────────┤\")\n",
        "    for idx, cite in enumerate(citations, 1):\n",
        "        chunk_id = cite.get('id', 'N/A')\n",
        "        relevance = cite.get('score', 0.0)\n",
        "        print(f\"│  {idx:<3} │ {chunk_id:<11} │ {relevance:>12.4f} │\")\n",
        "    print(\"└──────┴─────────────┴──────────────┘\")\n",
        "\n",
        "    print(f\"\\n{'─'*80}\\n\")\n",
        "\n",
        "print(\"\\n\" + \"─\" * 80)\n",
        "print(\"ALL TESTS COMPLETED\")\n",
        "print(\"─\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZJZ1z0uovA",
        "outputId": "6322ac81-7ed6-44b2-ee6d-62520c14e898"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "TESTING RAG SYSTEM - MULTIPLE QUERIES\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "================================================================================\n",
            "TEST QUERY #1\n",
            "================================================================================\n",
            "Query: Was Abraham Lincoln the sixteenth President of the United States?\n",
            "\n",
            "================================================================================\n",
            "Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
            "================================================================================\n",
            "\n",
            "Query variations (2):\n",
            "  1. Was Abraham Lincoln the sixteenth President of the United States?\n",
            "  2. Abraham Lincoln was the sixteenth president of the United States.\n",
            "Retrieved 35 unique candidates\n",
            "Reranked to top 5\n",
            "\n",
            "Context assembled: 1936 chars from 4 chunks\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "RESULTS\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Answer: Abraham Lincoln (February 12, 1809 â April 15, 1865) was the sixteenth President of the United States, serving from March 4, 1861 until his assassination.\n",
            "\n",
            "Source Citations:\n",
            "┌──────┬─────────────┬──────────────┐\n",
            "│ Rank │  Chunk ID   │   Relevance  │\n",
            "├──────┼─────────────┼──────────────┤\n",
            "│  1   │ 278-0       │      10.3089 │\n",
            "│  2   │ 319-0       │       8.1220 │\n",
            "│  3   │ 198-0       │      -0.4495 │\n",
            "│  4   │ 383-0       │      -0.7348 │\n",
            "└──────┴─────────────┴──────────────┘\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TEST QUERY #2\n",
            "================================================================================\n",
            "Query: Did Lincoln sign the National Banking Act of 1863?\n",
            "\n",
            "================================================================================\n",
            "Question: Did Lincoln sign the National Banking Act of 1863?\n",
            "================================================================================\n",
            "\n",
            "Query variations (1):\n",
            "  1. Did Lincoln sign the National Banking Act of 1863?\n",
            "Retrieved 30 unique candidates\n",
            "Reranked to top 5\n",
            "\n",
            "Context assembled: 2009 chars from 4 chunks\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "RESULTS\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Answer: Yes.\n",
            "\n",
            "Source Citations:\n",
            "┌──────┬─────────────┬──────────────┐\n",
            "│ Rank │  Chunk ID   │   Relevance  │\n",
            "├──────┼─────────────┼──────────────┤\n",
            "│  1   │ 360-1       │       4.8125 │\n",
            "│  2   │ 360-0       │       2.1481 │\n",
            "│  3   │ 330-0       │      -1.6246 │\n",
            "│  4   │ 352-1       │      -2.1655 │\n",
            "└──────┴─────────────┴──────────────┘\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "ALL TESTS COMPLETED\n",
            "────────────────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ]
    }
  ]
}